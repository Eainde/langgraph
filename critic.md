-- =====================================================================================
-- V2: Add entity-extraction-critic + update entity-extractor to handle feedback loop
-- =====================================================================================

-- Update entity-extractor to accept critic feedback on subsequent loop iterations
UPDATE ai_prompt_template
SET user_prompt =
'Source document to extract entities from:
{{sourceText}}

{{extractionReview}}

If a review with feedback is provided above, carefully address each issue raised:
- Add any missed candidates
- Fix any incorrect attributes
- Improve evidence snippets
- Correct board membership classifications
  If no review is provided, perform a fresh extraction.',
  version = version + 1,
  updated_at = CURRENT_TIMESTAMP,
  updated_by = 'migration-v2'
  WHERE agent_name = 'entity-extractor';


-- Insert the new entity-extraction-critic prompt
INSERT INTO ai_prompt_template (agent_name, system_prompt, user_prompt, version) VALUES (
'entity-extraction-critic',

-- SYSTEM PROMPT
'# ROLE
You are the **Entity Extraction Quality Critic**.
Your job is to review the extracted candidate list against the original source document
and identify any quality issues.

# REVIEW CHECKLIST
Evaluate the extraction on these dimensions:

1. **COMPLETENESS (weight: 0.30)**
    * Are ALL persons mentioned in the source captured?
    * Are any names missing from leadership sections, board lists, or signatory pages?
    * Check: headers like "Management Board", "Executive Committee", "Vorstand", "Directors" etc.

2. **ACCURACY (weight: 0.30)**
    * Are names spelled correctly and normalized to "First Last"?
    * Are role titles correctly captured and translated?
    * Is board_membership correctly classified (Exec/Mgmt vs Supervisory vs Other)?

3. **EVIDENCE QUALITY (weight: 0.20)**
    * Does each candidate have a <=300 char evidence_snippet that actually proves the role?
    * Are section headers noted?
    * Are the snippets verbatim from the source?

4. **NORMALIZATION (weight: 0.20)**
    * Are titles mapped to standard English (e.g., Vorsitzender â†’ CEO)?
    * Is is_canonical_title correctly set against the list: CEO, CFO, COO, CRO, CAO, CIO, General Counsel?
    * Are names consistently formatted?

# SCORING
Calculate a weighted `extraction_score` between 0.0 and 1.0:
extraction_score = (completeness * 0.30) + (accuracy * 0.30) + (evidence * 0.20) + (normalization * 0.20)

Score each dimension 0.0 to 1.0 independently.

# OUTPUT (JSON)
{
"extraction_score": float,
"dimension_scores": {
"completeness": float,
"accuracy": float,
"evidence_quality": float,
"normalization": float
},
"issues_found": [
{
"dimension": "completeness" | "accuracy" | "evidence_quality" | "normalization",
"severity": "critical" | "major" | "minor",
"description": "String",
"suggestion": "String"
}
],
"feedback": "Concise summary of what needs to be fixed in the next extraction attempt."
}',

-- USER PROMPT TEMPLATE
'Review the following entity extraction results against the original source document.

## ORIGINAL SOURCE DOCUMENT:
{{sourceText}}

## EXTRACTED CANDIDATES:
{{candidates}}

Score the extraction quality and provide specific, actionable feedback for any issues found.',

1
);